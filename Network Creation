{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations \n",
    "import random\n",
    "import networkx as nx\n",
    "#import EoN\n",
    "from concurrent import futures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a graph made of disconnected complete subgraphs, families, with Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Half Quarantine Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is graph connected?: False\n",
      "Number of components: 25000\n",
      "Mean of Number of Nodes in a Component: 2.52\n",
      "Average Clustering Coefficient: 0.6801489411217129\n",
      "Wall time: 1.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "is_half_quarantine = True\n",
    "\n",
    "G = nx.Graph()\n",
    "n_Of_Houses = 25000\n",
    "number_of_houses_in_neighborhood = 1000\n",
    "beta = 0.5\n",
    "mean_of_family_size = 2.52\n",
    "std_of_family_size = 1\n",
    "\n",
    "if is_half_quarantine:\n",
    "    house_white_blue_allocator = 0.02 # 0.02 for corona world. 0.93 for normal world\n",
    "    ratio_of_working_people = 0.21\n",
    "else:\n",
    "    house_white_blue_allocator = 0.93\n",
    "    ratio_of_working_people = 0.76\n",
    "    \n",
    "    \n",
    "size_of_Houses = []\n",
    "for i in range(n_Of_Houses):\n",
    "    number = round(random.gauss(mean_of_family_size, std_of_family_size))\n",
    "    while (number < 1):\n",
    "        number = round(random.gauss(mean_of_family_size, std_of_family_size))\n",
    "        \n",
    "    size_of_Houses.append(number)\n",
    "    if number == 1:\n",
    "        G.add_node(len(G))\n",
    "    else:\n",
    "        for pair in list(combinations(np.arange(len(G), len(G) + number) , 2)):\n",
    "            G.add_edge(pair[0], pair[1], weight = beta)\n",
    "print(\"Is graph connected?: \" + str(nx.is_connected(G)))\n",
    "print(\"Number of components: \" + str (nx.number_connected_components(G)))\n",
    "print(\"Mean of Number of Nodes in a Component: \" + str(mean_of_family_size))\n",
    "print(\"Average Clustering Coefficient: \" + str(nx.average_clustering(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is graph connected?: False\n",
      "Number of Components: 14263\n",
      "Mean of Number of Nodes in a Component: 4.519035266073056\n",
      "Average Clustering Coefficient: 0.6886030286831888\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# List of Nodes and Houses\n",
    "nodes = list(G.nodes())\n",
    "df = pd.DataFrame(nodes)\n",
    "df.columns = ['Node']\n",
    "\n",
    "lst = []\n",
    "for i in range(len(size_of_Houses)):\n",
    "    for j in range(size_of_Houses[i]):\n",
    "        lst.append(i)\n",
    "        \n",
    "df['House'] = lst\n",
    "\n",
    "house_List = np.arange(0, len(size_of_Houses), 1)\n",
    "\n",
    "# WHITE-BLUE\n",
    "lst_White = []\n",
    "white_blue_likelihood = 0.8\n",
    "for house in size_of_Houses:\n",
    "    is_White = np.random.rand() > house_white_blue_allocator\n",
    "    \n",
    "    for i in range(house):\n",
    "        if is_White:\n",
    "            lst_White.append(np.random.rand() < white_blue_likelihood)\n",
    "        else:\n",
    "            lst_White.append(np.random.rand() < (1 - white_blue_likelihood))\n",
    "            \n",
    "df['isWhite'] = lst_White\n",
    "\n",
    "# Send people to work\n",
    "df['Work'] = -1\n",
    "\n",
    "lst_of_jobs = []\n",
    "mean_of_job_size = 10\n",
    "std_of_job_size = 5\n",
    "while np.sum(lst_of_jobs) < df[df['isWhite'] == False].shape[0]:\n",
    "    job_size = round(random.gauss(mean_of_job_size, std_of_job_size))\n",
    "    if job_size < 2:\n",
    "        continue\n",
    "    lst_of_jobs.append(job_size)\n",
    "df_jobs = pd.DataFrame(lst_of_jobs).reset_index()\n",
    "df_jobs.columns = ['Job', 'No_of_Ppl']\n",
    "\n",
    "l = int(number_of_houses_in_neighborhood * mean_of_family_size / 2)\n",
    "\n",
    "mean_of_job_allocator = 0\n",
    "std_of_job_allocator = df_jobs.shape[0] / 4\n",
    "\n",
    "\n",
    "for node in df[df['isWhite'] == False]['Node'].values:\n",
    "    list_of_available_jobs = df_jobs[df_jobs['No_of_Ppl'] != 0]['Job'].values\n",
    "    corresponding_job_for_node = (node * ratio_of_working_people) / mean_of_job_size\n",
    "    job_allocation = min(list_of_available_jobs, key=lambda x:abs(x - (corresponding_job_for_node + round(random.gauss(mean_of_job_allocator, std_of_job_allocator)))))\n",
    "    df_jobs.iloc[job_allocation, 1] -= 1\n",
    "    df.iloc[node, 3] = job_allocation\n",
    "    \n",
    "# Add edges with  ð›½  for people at same work\n",
    "\n",
    "df_workers = df[df['isWhite'] == False]\n",
    "for item in df_workers[['Node', 'Work']].groupby('Work'):\n",
    "    for pair in list(combinations(item[1]['Node'].values , 2)):\n",
    "        G.add_edge(pair[0], pair[1], weight = beta)\n",
    "        \n",
    "print(\"Is graph connected?: \" + str(nx.is_connected(G)))\n",
    "c = [list(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "lst_of_sizes_of_c = []\n",
    "for comp in c:\n",
    "    lst_of_sizes_of_c.append(len(comp))\n",
    "print(\"Number of Components: \" + str(len(c)))\n",
    "print(\"Mean of Number of Nodes in a Component: \" + str(np.mean(lst_of_sizes_of_c)))\n",
    "print(\"Average Clustering Coefficient: \" + str(nx.average_clustering(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\beta^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50 % of Blue group are service industry such as couriers, market cashiers, etc that have to interact with a lot of people daily.\n",
    "- They are connected to 5% of the neighborhood where their work is at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is graph connected?: False\n",
      "Number of Components: 140\n",
      "Mean of Number of Nodes in a Component: 460.39285714285717\n",
      "Average Clustering Coefficient: 0.07133623448321635\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if is_half_quarantine:\n",
    "    ratio_of_service_industry = 0.50\n",
    "else:\n",
    "    ratio_of_service_industry = 0.33\n",
    "ratio_of_connections_for_service_industry = 0.05\n",
    "servers = df[df['isWhite'] == False].sample(frac = ratio_of_service_industry)['Node'].values\n",
    "\n",
    "for server in servers:\n",
    "    servers_corresponding_node_in_work_neighborhood = int((df.loc[server, 'Work'] * mean_of_job_size) / ratio_of_working_people)\n",
    "    to_be_served_by_current_server = df.iloc[servers_corresponding_node_in_work_neighborhood - l:servers_corresponding_node_in_work_neighborhood + l, :].sample(frac = ratio_of_connections_for_service_industry)['Node'].values\n",
    "    for to_be_served in to_be_served_by_current_server:\n",
    "        if G.has_edge(server, to_be_served) == False:\n",
    "            G.add_edge(server, to_be_served, weight = beta ** 2)\n",
    "\n",
    "print(\"Is graph connected?: \" + str(nx.is_connected(G)))\n",
    "c = [list(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "lst_of_sizes_of_c = []\n",
    "for comp in c:\n",
    "    lst_of_sizes_of_c.append(len(comp))\n",
    "print(\"Number of Components: \" + str(len(c)))\n",
    "print(\"Mean of Number of Nodes in a Component: \" + str(np.mean(lst_of_sizes_of_c)))\n",
    "print(\"Average Clustering Coefficient: \" + str(nx.average_clustering(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- People not only interact with people at work, or Blue group in service industry but also have small interactions with random people within neighborhood either by passing by in street, market, pharmacy line, etc.\n",
    "- Number of randomly contacted people is also determined for each person, according to Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\beta^{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is graph connected?: True\n",
      "Number of Components: 1\n",
      "Mean of Number of Nodes in a Component: 64455.0\n",
      "Average Clustering Coefficient: 0.03592646601109888\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mean_of_b_3_connections = int(number_of_houses_in_neighborhood * mean_of_family_size *  0.02)\n",
    "std_of_b_3_connections = mean_of_b_3_connections / 2\n",
    "for node in df['Node'].values:\n",
    "    \n",
    "    number_of_b_3_connections = max(0, round(random.gauss(mean_of_b_3_connections, std_of_b_3_connections)))\n",
    "    b_3_list = df.iloc[max(0, node-l):node+l, 0].sample(number_of_b_3_connections).values\n",
    "\n",
    "    for random_person in b_3_list:\n",
    "        if G.has_edge(node, random_person) == False:\n",
    "            G.add_edge(node, random_person, weight = beta**3)         \n",
    "            \n",
    "print(\"Is graph connected?: \" + str(nx.is_connected(G)))\n",
    "c = [list(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "lst_of_sizes_of_c = []\n",
    "for comp in c:\n",
    "    lst_of_sizes_of_c.append(len(comp))\n",
    "print(\"Number of Components: \" + str(len(c)))\n",
    "print(\"Mean of Number of Nodes in a Component: \" + str(np.mean(lst_of_sizes_of_c)))\n",
    "print(\"Average Clustering Coefficient: \" + str(nx.average_clustering(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nx.diameter(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, \"Half_Quarantine_Graph_USA.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_Half_Quarantine_Graph_USA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- World Before Covid-19 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friendship Relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each person has a number of friends with Gaussian distribution\n",
    "- Their location in the graph can be anywhere with mean 0\n",
    "- To have a wide range, std of friend location in the graph will be number of Nodes / 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df['Number_of_Friends_so_far'] = 0\n",
    "df['Number_of_Friends_defined'] = 0\n",
    "mean_of_number_of_friends = 6.5\n",
    "std_of_number_of_friends = 3\n",
    "lst_friendlist = []\n",
    "for node in df['Node'].values:\n",
    "    number_of_friends = max(0, round(random.gauss(mean_of_number_of_friends, std_of_number_of_friends)))\n",
    "    df.loc[node, 'Number_of_Friends_defined'] = number_of_friends\n",
    "\n",
    "while df[df['Number_of_Friends_so_far'] < df['Number_of_Friends_defined']].shape[0]>1:\n",
    "    node = df[df['Number_of_Friends_so_far'] < df['Number_of_Friends_defined']]['Node'].sample(1).values[0]\n",
    "    #friendlist_of_node = \"\"\n",
    "    while df.loc[node, 'Number_of_Friends_so_far'] < df.loc[node, 'Number_of_Friends_defined']:\n",
    "        some_std = max(0, round(random.gauss(0, df.shape[0] / 2 )))\n",
    "        while df[df['Number_of_Friends_so_far'] < df['Number_of_Friends_defined']].iloc[max(0, node - (l + some_std)): node + (l + some_std)].shape[0] < 1:\n",
    "            some_std = (some_std + 5)*2\n",
    "        friendNode = df[df['Number_of_Friends_so_far'] < df['Number_of_Friends_defined']].iloc[max(0, node - (l + some_std)): node + (l + some_std)]['Node'].sample(1).values[0]\n",
    "        if df.loc[friendNode, 'Number_of_Friends_so_far'] < df.loc[friendNode, 'Number_of_Friends_defined']:\n",
    "            df.loc[node, 'Number_of_Friends_so_far'] += 1\n",
    "            df.loc[friendNode, 'Number_of_Friends_so_far'] += 1\n",
    "            G.add_edge(node, friendNode, weight = beta)       \n",
    "    \n",
    "    # Weak interactions, 10x the number of friends, 10% being beta**2\n",
    "    nodes_of_weak_interactions = df['Node'].sample(10 * number_of_friends).values\n",
    "    for weak_interaction_node in nodes_of_weak_interactions:\n",
    "        if np.random.rand() < 0.1:\n",
    "            if G.has_edge(node, weak_interaction_node) == False:\n",
    "                G.add_edge(node, weak_interaction_node, weight = beta**2)      \n",
    "        else:\n",
    "            if G.has_edge(node, weak_interaction_node) == False:\n",
    "                G.add_edge(node, weak_interaction_node, weight = beta**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, \"World_Before_COVID19_Graph_USA.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('df_World_Before_COVID19_Graph_USA.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
